{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning of the approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_classifier(clf_name):\n",
    "    if clf_name == 'LogR':\n",
    "        return LogisticRegression(max_iter=1500000, random_state=24091993)\n",
    "    elif clf_name == 'DT':\n",
    "        return DecisionTreeClassifier(random_state=24091993)\n",
    "    elif clf_name == 'RF':\n",
    "        return RandomForestClassifier(random_state=24091993)\n",
    "    \n",
    "\n",
    "def select_params(clf_name):\n",
    "    if clf_name == 'LogR':\n",
    "        return {'C': np.logspace(-3,2, num=10),}\n",
    "    elif clf_name == 'LinSVM':\n",
    "        return {'C': np.logspace(-3,2, num=10), }\n",
    "    elif clf_name == 'DT':\n",
    "        return {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "    elif clf_name == 'RF':\n",
    "        return {'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'criterion' :['gini', 'entropy']}\n",
    "\n",
    "\n",
    "def create_features(data, window_size=2):\n",
    "    data['date'] = data['date'].astype('datetime64[ns]')\n",
    "    data['weekday'] = data['date'].dt.weekday\n",
    "    \n",
    "    # add a new column with the average value of the last two rows of other column\n",
    "    wl_sum = 0\n",
    "    sp_sum = 0\n",
    "    voice_sum = 0\n",
    "    wl_values = []\n",
    "    sp_values = []\n",
    "    voice_values = []\n",
    "    wl_delta = pd.Series(dtype='float64')\n",
    "    sp_delta = pd.Series(dtype='float64')\n",
    "    voice_delta = pd.Series(dtype='float64')\n",
    "    for i in range(0, window_size):\n",
    "        workload = data.groupby('user')['workload'].shift(i)\n",
    "        sleep = data.groupby('user')['sleep'].shift(i)\n",
    "        voice = data.groupby('user')['voice'].shift(i)\n",
    "        wl_sum += workload\n",
    "        wl_values.append(workload)\n",
    "        sp_sum += sleep\n",
    "        sp_values.append(sleep)\n",
    "        voice_sum += voice\n",
    "        voice_values.append(voice)\n",
    "        wl_delta = workload if wl_delta.empty else wl_delta - workload\n",
    "        sp_delta = sleep if sp_delta.empty else sp_delta - sleep\n",
    "        voice_delta = voice if voice_delta.empty else voice_delta - voice\n",
    "\n",
    "    data['prev_stress'] = data.groupby('user')['stress'].shift(1)\n",
    "\n",
    "    if window_size > 1:\n",
    "        data['wl_avg_{}'.format(window_size)] = wl_sum / window_size\n",
    "        data['sp_avg_{}'.format(window_size)] = sp_sum / window_size\n",
    "        data['wl_std_{}'.format(window_size)] = np.std(wl_values, axis=0)\n",
    "        data['sp_std_{}'.format(window_size)] = np.std(sp_values, axis=0)\n",
    "        data['wl_max_{}'.format(window_size)] = np.max(wl_values, axis=0)\n",
    "        data['sp_max_{}'.format(window_size)] = np.max(sp_values, axis=0)\n",
    "        data['wl_min_{}'.format(window_size)] = np.min(wl_values, axis=0)\n",
    "        data['sp_min_{}'.format(window_size)] = np.min(sp_values, axis=0)\n",
    "        data['wl_delta_{}'.format(window_size)] = wl_delta\n",
    "        data['sp_delta_{}'.format(window_size)] = sp_delta\n",
    "        data['voice_avg_{}'.format(window_size)] = voice_sum / window_size\n",
    "        data['voice_std_{}'.format(window_size)] = np.std(voice_values, axis=0)\n",
    "        data['voice_max_{}'.format(window_size)] = np.max(voice_values, axis=0)\n",
    "        data['voice_min_{}'.format(window_size)] = np.min(voice_values, axis=0)\n",
    "        data['voice_delta_{}'.format(window_size)] = voice_delta\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_missing_percentages = [30, 45, 60, 75]\n",
    "data_strategies = ['balanced', 'reliable']\n",
    "\n",
    "datasets = {\n",
    "    'sl' : {\n",
    "        'cut_strategies' : [''],\n",
    "        'weeks' : {\n",
    "            'balanced' : {\n",
    "                'start' : 13,\n",
    "                'length' : 7,\n",
    "                'train_lengths': [5, 6] \n",
    "            },\n",
    "            'reliable' : {\n",
    "                'start' : 13,\n",
    "                'length' : 4,\n",
    "                'train_lengths': [2, 3] \n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'fbk' : {\n",
    "        'cut_strategies' : ['_cut', '_notcut'],\n",
    "        'weeks' : {\n",
    "            'balanced' : {\n",
    "                'start' : 45,\n",
    "                'length' : 7,\n",
    "                'train_lengths': [5, 6] \n",
    "            },\n",
    "            'reliable' : {\n",
    "                'start' : 46,\n",
    "                'length' : 5,\n",
    "                'train_lengths': [3, 4] \n",
    "            },\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_name, cut_strategy, max_missing_percentage, data_strategy, train_length, dataset):\n",
    "    scores = []\n",
    "    for clf_name in ['LogR', 'DT', 'RF']:\n",
    "        np.random.seed(24091993)\n",
    "        clf = select_classifier(clf_name)\n",
    "        params = select_params(clf_name)\n",
    "        \n",
    "        data = dataset.copy()\n",
    "        \n",
    "        # training_data = data[(data['week'] >= data_strategy['start']) and (data['week'] < data_strategy['start'] + train_length - 1)]\n",
    "        # test_data = data[(data['week'] >= data_strategy['start'] + train_length) and (data['week'] < data_strategy['start'] + data_strategy['length'])]\n",
    "\n",
    "        rscv = GridSearchCV(clf, params, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "        rscv.fit(data.drop(['stress', 'date', 'duration', 'user'], axis=1), data['stress'])\n",
    "        scores.append([dataset_name, clf_name, str(rscv.best_params_), rscv.best_score_,  max_missing_percentage, data_strategy, train_length, cut_strategy])\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_array = []\n",
    "for dataset_name in datasets:\n",
    "    for cut_strategy in datasets[dataset_name]['cut_strategies']:\n",
    "        for max_missing_percentage in max_missing_percentages:\n",
    "            for data_strategy in data_strategies:\n",
    "                dataframe = pd.read_csv('../data/processed/{}_processed_data_{}_{}{}.csv'.format(dataset_name, max_missing_percentage, data_strategy, cut_strategy))\n",
    "                for train_length in datasets[dataset_name]['weeks'][data_strategy]['train_lengths']:\n",
    "                    data = dataframe.copy()\n",
    "                    data = create_features(data, window_size=2)\n",
    "                    data.dropna(inplace=True)\n",
    "                    scores = evaluate(dataset_name, cut_strategy, max_missing_percentage, data_strategy, train_length, data)\n",
    "                    scores = pd.DataFrame(scores, columns=['dataset', 'classifier', 'params', 'score', 'max_missing_percentage', 'data_strategy', 'train_length', 'cut_strategy'])\n",
    "                    scores_array.append(scores)\n",
    "scores = pd.concat(scores_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>classifier</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "      <th>max_missing_percentage</th>\n",
       "      <th>data_strategy</th>\n",
       "      <th>train_length</th>\n",
       "      <th>cut_strategy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sl</td>\n",
       "      <td>LogR</td>\n",
       "      <td>{'C': 7.742636826811278}</td>\n",
       "      <td>0.681652</td>\n",
       "      <td>30</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sl</td>\n",
       "      <td>DT</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_leaf': 4, 'min_s...</td>\n",
       "      <td>0.661957</td>\n",
       "      <td>30</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sl</td>\n",
       "      <td>RF</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'n_es...</td>\n",
       "      <td>0.634313</td>\n",
       "      <td>30</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sl</td>\n",
       "      <td>LogR</td>\n",
       "      <td>{'C': 7.742636826811278}</td>\n",
       "      <td>0.681652</td>\n",
       "      <td>30</td>\n",
       "      <td>balanced</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sl</td>\n",
       "      <td>DT</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_leaf': 4, 'min_s...</td>\n",
       "      <td>0.661957</td>\n",
       "      <td>30</td>\n",
       "      <td>balanced</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fbk</td>\n",
       "      <td>DT</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 1, 'min_s...</td>\n",
       "      <td>0.736233</td>\n",
       "      <td>75</td>\n",
       "      <td>reliable</td>\n",
       "      <td>3</td>\n",
       "      <td>_notcut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fbk</td>\n",
       "      <td>RF</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'n_estim...</td>\n",
       "      <td>0.723822</td>\n",
       "      <td>75</td>\n",
       "      <td>reliable</td>\n",
       "      <td>3</td>\n",
       "      <td>_notcut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fbk</td>\n",
       "      <td>LogR</td>\n",
       "      <td>{'C': 7.742636826811278}</td>\n",
       "      <td>0.730231</td>\n",
       "      <td>75</td>\n",
       "      <td>reliable</td>\n",
       "      <td>4</td>\n",
       "      <td>_notcut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fbk</td>\n",
       "      <td>DT</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 1, 'min_s...</td>\n",
       "      <td>0.736233</td>\n",
       "      <td>75</td>\n",
       "      <td>reliable</td>\n",
       "      <td>4</td>\n",
       "      <td>_notcut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fbk</td>\n",
       "      <td>RF</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'n_estim...</td>\n",
       "      <td>0.723822</td>\n",
       "      <td>75</td>\n",
       "      <td>reliable</td>\n",
       "      <td>4</td>\n",
       "      <td>_notcut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset classifier                                             params  \\\n",
       "0       sl       LogR                           {'C': 7.742636826811278}   \n",
       "1       sl         DT  {'max_depth': 3, 'min_samples_leaf': 4, 'min_s...   \n",
       "2       sl         RF  {'criterion': 'entropy', 'max_depth': 4, 'n_es...   \n",
       "0       sl       LogR                           {'C': 7.742636826811278}   \n",
       "1       sl         DT  {'max_depth': 3, 'min_samples_leaf': 4, 'min_s...   \n",
       "..     ...        ...                                                ...   \n",
       "1      fbk         DT  {'max_depth': 2, 'min_samples_leaf': 1, 'min_s...   \n",
       "2      fbk         RF  {'criterion': 'gini', 'max_depth': 6, 'n_estim...   \n",
       "0      fbk       LogR                           {'C': 7.742636826811278}   \n",
       "1      fbk         DT  {'max_depth': 2, 'min_samples_leaf': 1, 'min_s...   \n",
       "2      fbk         RF  {'criterion': 'gini', 'max_depth': 6, 'n_estim...   \n",
       "\n",
       "       score  max_missing_percentage data_strategy  train_length cut_strategy  \n",
       "0   0.681652                      30      balanced             5               \n",
       "1   0.661957                      30      balanced             5               \n",
       "2   0.634313                      30      balanced             5               \n",
       "0   0.681652                      30      balanced             6               \n",
       "1   0.661957                      30      balanced             6               \n",
       "..       ...                     ...           ...           ...          ...  \n",
       "1   0.736233                      75      reliable             3      _notcut  \n",
       "2   0.723822                      75      reliable             3      _notcut  \n",
       "0   0.730231                      75      reliable             4      _notcut  \n",
       "1   0.736233                      75      reliable             4      _notcut  \n",
       "2   0.723822                      75      reliable             4      _notcut  \n",
       "\n",
       "[144 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'max_depth': 4, 'min_samples_leaf': 10, 'min_samples_split': 2}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[scores['dataset'] == 'fbk'].query('score == score.max()').iloc[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>classifier</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "      <th>max_missing_percentage</th>\n",
       "      <th>data_strategy</th>\n",
       "      <th>train_length</th>\n",
       "      <th>cut_strategy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fbk</td>\n",
       "      <td>DT</td>\n",
       "      <td>{'max_depth': 4, 'min_samples_leaf': 10, 'min_...</td>\n",
       "      <td>0.77398</td>\n",
       "      <td>75</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>_cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fbk</td>\n",
       "      <td>DT</td>\n",
       "      <td>{'max_depth': 4, 'min_samples_leaf': 10, 'min_...</td>\n",
       "      <td>0.77398</td>\n",
       "      <td>75</td>\n",
       "      <td>balanced</td>\n",
       "      <td>6</td>\n",
       "      <td>_cut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset classifier                                             params  \\\n",
       "1     fbk         DT  {'max_depth': 4, 'min_samples_leaf': 10, 'min_...   \n",
       "1     fbk         DT  {'max_depth': 4, 'min_samples_leaf': 10, 'min_...   \n",
       "\n",
       "     score  max_missing_percentage data_strategy  train_length cut_strategy  \n",
       "1  0.77398                      75      balanced             5         _cut  \n",
       "1  0.77398                      75      balanced             6         _cut  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[scores['dataset'] == 'fbk'].query('score == score.max()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[scores['dataset'] == 'sl'].query('score == score.max()').iloc[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>classifier</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "      <th>max_missing_percentage</th>\n",
       "      <th>data_strategy</th>\n",
       "      <th>train_length</th>\n",
       "      <th>cut_strategy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sl</td>\n",
       "      <td>DT</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 1, 'min_s...</td>\n",
       "      <td>0.782199</td>\n",
       "      <td>75</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sl</td>\n",
       "      <td>DT</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 1, 'min_s...</td>\n",
       "      <td>0.782199</td>\n",
       "      <td>75</td>\n",
       "      <td>balanced</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset classifier                                             params  \\\n",
       "1      sl         DT  {'max_depth': 2, 'min_samples_leaf': 1, 'min_s...   \n",
       "1      sl         DT  {'max_depth': 2, 'min_samples_leaf': 1, 'min_s...   \n",
       "\n",
       "      score  max_missing_percentage data_strategy  train_length cut_strategy  \n",
       "1  0.782199                      75      balanced             5               \n",
       "1  0.782199                      75      balanced             6               "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[scores['dataset'] == 'sl'].query('score == score.max()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with the fine-tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.86      0.87        50\n",
      "           2       0.92      0.91      0.92       148\n",
      "           3       0.87      0.91      0.89        68\n",
      "\n",
      "    accuracy                           0.90       266\n",
      "   macro avg       0.89      0.89      0.89       266\n",
      "weighted avg       0.90      0.90      0.90       266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=24091993,  min_samples_split=2, min_samples_leaf=1, max_depth=2)\n",
    "data = pd.read_csv('../data/processed/sl_processed_data_75_balanced.csv')\n",
    "data = create_features(data, window_size=2)\n",
    "data.dropna(inplace=True)\n",
    "data['week'] = data['date'].astype('datetime64[ns]').dt.isocalendar().week\n",
    "training_data = data[(data['week'] >= 13) & (data['week'] <= 13 + 6 - 1)]\n",
    "test_data = data[(data['week'] >= 13 + 6) & (data['week'] < 13 + 7)]\n",
    "clf.fit(training_data.drop(['stress', 'date', 'duration', 'user', 'week'], axis=1).values, training_data['stress'])\n",
    "y_pred = clf.predict(test_data.drop(['stress', 'date', 'duration', 'user', 'week'], axis=1).values)\n",
    "y_true = test_data['stress']\n",
    "print(classification_report(y_true, y_pred))\n",
    "pickle.dump(clf, open('../trained_models/sl_data_75_balanced_model_trained.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.73      0.73       381\n",
      "           2       0.77      0.77      0.77       725\n",
      "           3       0.79      0.77      0.78       452\n",
      "\n",
      "    accuracy                           0.76      1558\n",
      "   macro avg       0.76      0.76      0.76      1558\n",
      "weighted avg       0.76      0.76      0.76      1558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(training_data['stress'], clf.predict(training_data.drop(['stress', 'date', 'duration', 'user', 'week'], axis=1).values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf \u001b[39m=\u001b[39m DecisionTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m24091993\u001b[39m,  min_samples_split\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, min_samples_leaf\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m      2\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/processed/fbk_processed_data_75_balanced_cut.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m data \u001b[39m=\u001b[39m create_features(data, window_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=24091993,  min_samples_split=2, min_samples_leaf=10, max_depth=4)\n",
    "data = pd.read_csv('../data/processed/fbk_processed_data_75_balanced_cut.csv')\n",
    "data = create_features(data, window_size=2)\n",
    "data.dropna(inplace=True)\n",
    "data['week'] = data['date'].astype('datetime64[ns]').dt.isocalendar().week\n",
    "training_data = data[(data['week'] >= 45) & (data['week'] <= 45 + 6 - 1)]\n",
    "test_data = data[(data['week'] >= 45 + 6) & (data['week'] < 45 + 7)]\n",
    "clf.fit(training_data.drop(['stress', 'date', 'duration', 'user', 'week'], axis=1).values, training_data['stress'])\n",
    "y_pred = clf.predict(test_data.drop(['stress', 'date', 'duration', 'user', 'week'], axis=1).values)\n",
    "y_true = test_data['stress']\n",
    "print(classification_report(y_true, y_pred))\n",
    "pickle.dump(clf, open('../trained_models/fbk_data_75_balanced_cut_model_trained.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.88      0.86       649\n",
      "           2       0.68      0.68      0.68       356\n",
      "           3       0.77      0.67      0.72       225\n",
      "\n",
      "    accuracy                           0.78      1230\n",
      "   macro avg       0.76      0.74      0.75      1230\n",
      "weighted avg       0.78      0.78      0.78      1230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(training_data['stress'], clf.predict(training_data.drop(['stress', 'date', 'duration', 'user', 'week'], axis=1).values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvabmpaams23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b35f28ca8490582ff8f77ff9980f59b5ef0ec07a74ec2efd5c7bfd81cb9aec3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
